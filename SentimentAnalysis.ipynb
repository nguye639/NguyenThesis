{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from sentistrength import PySentiStr\n",
    "from nltk.util import trigrams\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\"\n",
    "from threading import Thread\n",
    "import queue\n",
    "import time\n",
    "from csv import writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Le\\AppData\\Local\\Temp\\tmpss25nx4n\\config.json as plain json\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from AllenNLPModel import useAllen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Le\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def cleanData(messages):\n",
    "    for i in range(0,len(messages)):\n",
    "        try:\n",
    "            doc = nlp(messages.iloc[i])\n",
    "            token_list = [token for token in doc]\n",
    "            filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "            lemmas = [token.lemma_ for token in filtered_tokens]\n",
    "            s = ' '.join(lemmas)\n",
    "            \n",
    "            messages.iloc[i] = re.sub(r'\\W+', ' ', s)\n",
    "        except:\n",
    "            messages.iloc[i] = \"\"\n",
    "        \n",
    "        #if is_ascii(messages.iloc[i]) == False:\n",
    "            # messages.iloc[i] = \"\"\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath('C:/Users/Le/Documents/Thesis/Code/SentiStrength/SentiStrength.jar')\n",
    "senti.setSentiStrengthLanguageFolderPath('C:/Users/Le/Documents/Thesis/Code/SentiStrength/SentiStrength_Data')\n",
    "\n",
    "def useSentiStrength(message):\n",
    "    try:\n",
    "        result = np.sum(senti.getSentiment(message.split()))\n",
    "        result = result/np.sqrt(result**2 + 15)\n",
    "    except: \n",
    "        result = 0\n",
    "    \n",
    "    return  result\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def useNLTK(message):\n",
    "    try:\n",
    "        result = sia.polarity_scores(message)['compound']\n",
    "    except: \n",
    "        result == 0\n",
    "      \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Thread Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "#data = pd.read_csv(\"GawaherClean.csv\", index_col=0)\n",
    "data = pd.read_csv(\"CleanData/GawaherClean.csv\", index_col=0)\n",
    "\n",
    "#Get thread\n",
    "threads = data[\"ThreadID\"].unique()\n",
    "thread = data[data[\"ThreadID\"]== threads[0]]\n",
    "thread = thread.reset_index(drop = True)\n",
    "\n",
    "#get all messages\n",
    "messages = thread[\"Message\"].copy()\n",
    "\n",
    "clean = cleanData(messages.copy())\n",
    "print(len(clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiSentiment = []\n",
    "nltkSentiment = []\n",
    "allenSentiment = []\n",
    "\n",
    "counter = 0\n",
    "for i in clean:\n",
    "    sentiSentiment.append(useSentiStrength(i))\n",
    "    nltkSentiment.append(useNLTK(i))\n",
    "    allenSentiment.append(useAllen(messages.iloc[counter]))\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentData = pd.DataFrame(np.array([messages.to_numpy(),sentiSentiment,nltkSentiment,allenSentiment]).T,\n",
    "                             columns = [\"Messages\",\"SentiStrength\",\"NLTK\",\"Allen\"])\n",
    "\n",
    "sentimentData = sentimentData.sort_values([\"Allen\",\"SentiStrength\",\"NLTK\"])\n",
    "sentimentData = sentimentData.dropna()\n",
    "sentimentData = sentimentData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.hist(sentimentData[\"NLTK\"], alpha = .5, label = \"NLTK\", bins = 25)\n",
    "plt.hist(sentimentData[\"SentiStrength\"], alpha = .5, label = \"SentiStrength\", bins = 25)\n",
    "plt.hist(sentimentData[\"Allen\"], alpha = .5, label = \"Allen\", bins = 25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "test = messages.iloc[i]\n",
    "print(test + \"\\n\")\n",
    "\n",
    "print(\"NLTK: \" + str(useNLTK(test)))\n",
    "print(\"SentiStrength: \" + str(useSentiStrength(test)))\n",
    "print(\"AllenNLP (Bert): \" + str(useAllen(test)))\n",
    "\n",
    "#find paper on AllenNLP and how it performs better than other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def getSentiments(row):\n",
    "  \n",
    "    message = row[\"Message\"]\n",
    "    row[\"AllenNLP\"] = useAllen(message)\n",
    "    \n",
    "    message = cleanRow(message)\n",
    "    row[\"NLTK\"] = useNLTK(message)\n",
    "    row[\"Sentistrength\"] = useSentiStrength(message)\n",
    "    \n",
    "    with open('GawaherSentiment.csv', 'a', encoding=\"utf-8\") as f_object:\n",
    "        writer_object = writer(f_object)\n",
    "        writer_object.writerow(list(row))\n",
    "        f_object.close()\n",
    "    \n",
    "def cleanRow(message):\n",
    "    try:\n",
    "        doc = nlp(message)\n",
    "        token_list = [token for token in doc]\n",
    "        filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "        lemmas = [token.lemma_ for token in filtered_tokens]\n",
    "        s = ' '.join(lemmas)\n",
    "            \n",
    "        messages = re.sub(r'\\W+', ' ', s)\n",
    "    except:\n",
    "        messages = \"\"\n",
    "    \n",
    "    return messages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"CleanData/GawaherClean.csv\", index_col=0)\n",
    "\n",
    "data[\"NLTK\"] = np.zeros(len(data))\n",
    "data[\"Sentistrength\"] = np.zeros(len(data))\n",
    "data[\"AllenNLP\"] = np.zeros(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"GawaherSentiment.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2403"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "C:\\Users\\Le\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentistrength\\__init__.py:29: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_text = pd.Series(df_text)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(data)):\n",
    "    getSentiments(data.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#rows = queue.Queue()\n",
    "\n",
    "allThreads = []\n",
    "\n",
    "for i in range(2,len(data)):\n",
    "    row = Thread(target= getSentiments, args=(data,i))\n",
    "    allThreads.append(row)\n",
    "    \n",
    "for thread in allThreads:\n",
    "    thread.start()\n",
    "    \n",
    "for thread in allThreads:\n",
    "    thread.join()\n",
    "\n",
    "#rows.qsize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thread Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def getAllSentiment(thread):\n",
    "    messages = thread[\"Message\"].copy()\n",
    "    clean = cleanData(messages)\n",
    "    sentiment = []\n",
    "    for i in clean:\n",
    "        sentiment.append(useNLTK(i))\n",
    "    \n",
    "    return np.mean(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "#data = pd.read_csv(\"Ansar1Clean.csv\", index_col=0)\n",
    "data = pd.read_csv(\"CleanData/GawaherClean.csv\", index_col=0)\n",
    "\n",
    "#Get thread\n",
    "threads = data[\"ThreadID\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "\n",
    "for i in range(0,len(threads)):\n",
    "    thread = data[data[\"ThreadID\"]== threads[i]]\n",
    "    sentiments.append(getAllSentiment(thread))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(sentiments, bins = 25)\n",
    "plt.xlabel(\"Sentiment\", fontsize = 14)\n",
    "plt.ylabel(\"count\", fontsize = 14)\n",
    "plt.title(\"Sentiment Distribution of Cleaned Data\", fontsize = 14)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14);\n",
    "\n",
    "plt.savefig(\"Images/SentimentDistribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.array(sentiments) < 0)/len(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentData = pd.DataFrame(np.array([threads,sentiments]).T, columns = [\"thread\",\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentimentData.to_csv(\"sentimentData.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
